---
title: "Lesson 2: Synthetic Data Methods"
author: ""
date: "October 6, 2022"
output:
  html_document:
    number_sections: false
    self_contained: TRUE
    code_folding: hide
    toc: TRUE
    toc_float: TRUE
    css: !expr here::here("www", "web_report.css")
editor_options:
  chunk_output_type: console
bibliography: references.bib
---

```{r rmarkdown-setup, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

```{=html}
<style>
@import url('https://fonts.googleapis.com/css?family=Lato&display=swap');
</style>
```
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato" />

```{r header-image, fig.width = 5.14, fig.height = 1.46, echo = FALSE}
# All defaults
knitr::include_graphics(here::here('www', 'images', 'urban-institute-logo.png'))
```

<br> <br>

# Synthetic Data


**Synthetic data** consists of pseudo or “fake” records that are statistically representative of the confidential data. Records are considered **synthesized** when they are replaced with draws from a model fitted to the confidential data. 

- The goal of most synthesis is to closely mimic the underlying distribution and statistical properties of the real data to preserve data utility while minimizing disclosure risks.
- Synthesized values also limit an intruder's confidence, because they cannot confirm a synthetic value exists in the confidential dataset.
- Synthetic data may be used as a “training dataset” to develop programs to run on confidential data via a validation server.


**Partially synthetic** data only synthesizes some of the variables in the released data (generally those most sensitive to disclosure). In partially synthetic data, there remains a one-to-one mapping between confidential records and synthetic records. Below, we see an example of what a partially synthesized version of the above confidential data could look like.

![](images/partially-synthetic-data.png){width="437"}

**Fully synthetic** data synthesizes all values in the dataset with imputed amounts. Fully synthetic data no longer directly map onto the confidential records, but remain statistically representative. Since fully synthetic data does not contain any actual observations, it protects against both attribute and identity disclosure. Below, we see an example of what a fully synthesized version of the confidential data shown above could look like.


![](images/fully-synthetic-data.png){width="439"}

<br> <br>

# Synthetic Data <-> Imputation Connection

- Multiple imputation was originally developed to address non-response problems in surveys [@rubin1977formalizing].
- Statisticians created new observations or values to replace the missing data by developing a model based on other available respondent information.
- This process of replacing missing data with substituted values is called **imputation**.

## Imputation Example

```{r, echo = F, setup}
# load packages we need
library(tidyverse)
library(urbnthemes)
library(palmerpenguins)
library(kableExtra)
library(gt)
# Helper function to create nice gt tables
create_table <- function(data_df, 
                         rowname_col = NA,
                         fig_num = "",
                         title_text = ""){
  # random_id = random_id(n=10)
  random_id = "urban_table"
  
  basic_table = data_df %>% 
    gt(id = random_id, rowname_col = rowname_col) %>% 
    tab_options(#table.width = px(760),
                table.align = "left", 
                heading.align = "left",
                # TODO: Discuss with Comms whether border should extend across 
                # whole row at bottom or just across data cells
                table.border.top.style = "hidden",
                table.border.bottom.style = "transparent",
                heading.border.bottom.style = "hidden",
                # Need to set this to transparent so that cells_borders of the cells can display properly and 
                table_body.border.bottom.style = "transparent",
                table_body.border.top.style = "transparent",
                # column_labels.border.bottom.style = "transparent",
                column_labels.border.bottom.width = px(1),
                column_labels.border.bottom.color = "black",
                # row_group.border.top.style = "hidden",
                # Set font sizes
                heading.title.font.size = px(13),
                heading.subtitle.font.size = px(13),
                column_labels.font.size = px(13),
                table.font.size = px(13),
                source_notes.font.size = px(13),
                footnotes.font.size = px(13),
                # Set row group label and border options
                row_group.font.size = px(13),
                row_group.border.top.style = "transparent",
                row_group.border.bottom.style = "hidden",
                stub.border.style = "dashed",
                ) %>% 
    tab_header(
      title = fig_num,# "eyebrow",
      subtitle = title_text) %>%  #"Top 10 Banks (by Dollar Volume) for Community Development Lending") %>% 
    # Bold title, subtitle, and columns
    tab_style(
      style = cell_text(color = "black", weight = "bold", align = "left"),
      locations = cells_title("subtitle")
    ) %>% 
    tab_style(
      style = cell_text(color = "#696969", weight = "normal", align = "left", transform = "uppercase"),
      locations = cells_title("title")
    ) %>% 
    tab_style(
      style = cell_text(color = "black", weight = "bold", size = px(13)),
      locations = cells_column_labels(gt::everything())
    ) %>% 
    # Italicize row group and column spanner text
    tab_style(
      style = cell_text(color = "black", style = "italic", size  = px(13)),
      locations = gt::cells_row_groups()
    ) %>% 
    tab_style(
      style = cell_text(color = "black", style = "italic", size  = px(13)),
      locations = gt::cells_column_spanners()
    ) %>% 
    opt_table_font(
        font = list(
          google_font("Lato"),
          default_fonts()
        )
      ) %>% 
    # Adjust cell borders for all cells, small grey bottom border, no top border
    tab_style(
      style = list(
        cell_borders(
          sides = c("bottom"),
          color = "#d2d2d2",
          weight = px(1)
        )
      ),
      locations = list(
        cells_body(
          columns =  gt::everything()
          # rows = gt::everything()
        )
      )
    )  %>%
    tab_style(
      style = list(
        cell_borders(
          sides = c("top"),
          color = "#d2d2d2",
          weight = px(0)
        )
      ),
      locations = list(
        cells_body(
          columns =  gt::everything()
          # rows = gt::everything()
        )
      )
    )  %>%
    # Set missing value defaults
    fmt_missing(columns = gt::everything(), missing_text = "—") %>%
    # Set css for all the things we can't finetune exactly in gt, mostly t/r/b/l padding
    opt_css(
      css = str_glue("
      #{random_id} .gt_row {{
        padding: 5px 5px 5px 5px;
      }}
      #{random_id} .gt_sourcenote {{
        padding: 16px 0px 0px 0px;
      }}
      #{random_id} .gt_footnote {{
        padding: 16px 0px 0px 0px;
      }}
      #{random_id} .gt_subtitle {{
        padding: 0px 0px 2px 0px;
      }}
      #{random_id} .gt_col_heading {{
        padding: 10px 5px 10px 5px;
      }}
      #{random_id} .gt_col_headings {{
        padding: 0px 0px 0px 0px;
        border-top-width: 0px;
      }}
      #{random_id} .gt_group_heading {{
        padding: 15px 0px 0px 0px;
      }}
      #{random_id} .gt_stub {{
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: #d2d2d2;
        border-top-color: black;
        text-align: left;
      }}
      #{random_id} .gt_grand_summary_row {{
        border-bottom-width: 1px;
        border-top-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: #d2d2d2;
        border-top-color: #d2d2d2;
      }}
      #{random_id} .gt_summary_row {{
        border-bottom-width: 1px;
        border-top-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: #d2d2d2;
      }}
      #{random_id} .gt_column_spanner {{
        padding-top: 10px;
        padding-bottom: 10px;
      }}
      ") %>% as.character()
    )
  
  return(basic_table)
}
# set Urban Institute data visualization styles
set_urbn_defaults(base_size = 12)
# set a seed so pseudo-random processes are reproducible
set.seed(20220301)
# Create data of conference attendees, where half are missing age
sample_conf <- tibble(
  attendee_number = 1:80,
  age = c(round(rnorm(n = 40, mean = 46, sd = 13), 0), rep(NA, 40))
)
```

Imagine you are running a conference with 80 attendees. You are collecting names and ages of all your attendees. Unfortunately, when the conference is over, you realize that only about half of the attendees listed their ages. One common imputation technique is to just replace the missing values with the mean age of those in the data. 


Shown below is the distribution of the 40 age observations that are not missing.

```{r, echo = F, fig.height = 3.5, before_hist}
# plot attendee ages
ggplot(sample_conf, aes(x = age)) +
  geom_histogram(binwidth = 5, color = "white") + 
  labs(title = 'Histogram of attendee ages')
# replace NA values with mean age
sample_conf <- sample_conf %>%
  mutate(
    age = if_else(
      condition = is.na(age), 
      true = round(mean(age, na.rm = TRUE), 0), 
      false = age
    )
  )
```

And after imptuation, the histogram looks like this:

```{r, echo = F, fig.height = 3.5, after_hist}
# replot the histogram
ggplot(sample_conf, aes(x = age)) +
  geom_histogram(binwidth = 5, color = "white") + 
  labs(title = 'Histogram of attendee ages (with missing values imputed)')
```


-   Using the mean to impute the missing ages removes useful variation and conceals information from the "tails" of the distribution.
-   Another way to think about it was that we used a really straightforward model (just replace the data with the mean) and sampled from that model to fill in the missing values.
-   When creating synthetic data, this process is repeated for an entire variable, or set of variables
-   In a sense, the entire column is treated as missing!


<br> <br>

# Sequential Synthesis

A more advanced implementation of synthetic data generation estimates models for each predictor with previously synthesized variables used as predictors. This iterative process is called **sequential synthesis**. This allows us to easily model multivariate relationships (or joint distributions) without being computationally expensive


The process described above may be easier to understand with the following table:


```{r, echo = FALSE}
table = tribble(~Step, ~Outcome, ~`Modelled with`, ~`Predicted with`,
                "1", "Sex", NA, "Random sampling with replacement",
                "2", "Age", "Sex", "Sampled Sex",
                "3", "Social Security Benefits","Sex, Age" , "Sampled Sex, Sampled Age",
                NA, NA, NA, NA,
                )
table %>% 
  create_table() 
```

<br>
<br>


- We can select the synthesis order based on the priority of the variables or the relationships between them.
- The earlier in the order a variable is synthesized, the better the original information is preserved in the synthetic data **usually**.
- [@bowen2021differentially] proposed a method that ranks variable importance by either practical or statistical utility and sequentially synthesizes the data accordingly.

<br> <br>

# Parametric vs. Nonparametric Data Generation Process

**Parametric data synthesis** is the process of data generation based on a parametric distribution or generative model.

- Parametric models assume a finite number of parameters that capture the complexity of the data.

- They are generally less flexible, but more interpretable than nonparametric models.

- Examples: regression to assign an age variable, sampling from a probability distribution, Bayesian models, copula based models.

**Nonparametric data synthesis** is the process of data generation that is *not* based on assumptions about an underlying distribution or model.

- Often, nonparametric methods use frequency proportions or marginal probabilities as weights for some type of sampling scheme.

- They are generally more flexible, but less interpretable than parametric models.

- Examples: assigning gender based on underlying proportions, CART (Classification and Regression Trees) models, RNN models, etc.

**Important:** Synthetic data are only as good as the models used for imputation!

<br> <br>

# Implicates

-   Researchers can create any number of versions of a partially synthetic or fully synthetic dataset. Each version of the dataset is called an **implicate**. These can also be referred to as replicates or simply "synthetic datasets"

    -   For partially synthetic data, non-synthesized variables are the same across each version of the dataset

-   Multiple implicates are useful for understanding the uncertainty added by imputation and are required for calculating valid standard errors

-   More than one implicate can be released for public use; each new release, however, increases disclosure risk (but allows for more complete analysis and better inferences, provided users use the correct combining rules)

-   Implicates can also be analyzed internally to find which version(s) of the dataset provide the most utility in terms of data quality

<br> <br>

#  Exercise 1

### Sequential Synthesis {.tabset}

#### <font color="#55b748">**Question 1**</font>

You have a confidential dataset that contains information about dogs' `weight` and their `height`. You decide to sequentially synthesize these two variables and write up your method below. Can you spot the mistake in writing up your method?

> To create a synthetic record, first synthetic pet weight is assigned based on a random draw from a normal distribution with mean equal to the average of confidential weights, and standard deviation equal to the standard deviation of confidential weights. Then the confidential height is regressed on the synthetic weights. Using the resulting regression coefficients, a synthetic height variable is generated for each row in the data using just the synthetic `weight`s as an input.

<br> <br>

#### <font color="#55b748">**Question 1 Notes**</font>

You have a confidential dataset that contains information about dogs' `weight` and their `height`. You decide to sequentially synthesize these two variables and write up your method below. Can you spot the mistake in writing up your method?


> To create a synthetic record, first synthetic pet weight is assigned based on a random draw from a normal distribution with mean equal to the average of confidential weights, and standard deviation equal to the standard deviation of confidential weights. Then the confidential height is regressed on the synthetic weights. Using the resulting regression coefficients, a synthetic height variable is generated for each row in the data using just the synthetic `weight`s as an input.
<br> <br>

- **`Height` should be regressed on the confidential values for `weight`, rather than the synthetic values for `weight`**


<br> <br>

# Case Study

## Fully Synthetic SIPP data [@benedetto2018creation]

* **Data:** Survey of Income and Program Participation linked to administrative longitudinal earnings and benefits data from IRS and SSA.
* **Motivation:** To expand access to detailed economic data that is highly restricted without heavy disclosure control. 
* **Methods:** Sequential regression multiple imputation (SRMI) with OLS regression, logistic regression, and Bayesian bootstrap. They released four implicates of the synthetic data.
* **Important metrics:**
    * General utility: pMSE
    * Specific utility: None
    * Disclosure: Distance based re-identification, RMSE of the closest record to measure attribute disclosure
* **Lessons learned:**
    * One of the first major synthetic files in the US.
    * The file includes complex relationships between family members that are synthesized. 
    
## Partially Synthetic Geocodes [@drechsler2021synthesizing]

* **Data:** Integrated Employment Biographies (German administrative data) with linked geocodes (latitude and longitude) 
* **Motivation:** Rich geographic information can be used to answer many important labor market research questions. This data would otherwise would be too sensitive to release, due to the possibility of identifying an individual based on the combination of their location and other attributes.
* **Methods:** CART with categorical geocodes. Also evaluated CART with continuous geocodes and a Bayesian latent class model.
* **Important metrics:**
    * General utility: Relative frequencies of cross tabulations
    * Specific utility: Zip Code comparisons of tabulated variables, Ripley's K- and L-functions
    * Disclosure: Probabilities of re-identification [@reiter2009estimating] -> comparison of expected match risk and the true match rate
* **Lessons learned:**
    * The synthetic data with geocodes had more measured disclosure risk than the original data. 
    * Synthesizing more variables made a huge difference in the measured disclosure risks.
    * Adjusting CART hyperparameters was not an effective way to manage the risk-utility tradeoff. 
    * They stratified the data before synthesis for computational reasons. 
    
# Suggested Reading

Snoke, J., Raab, G. M., Nowok, B., Dibben, C., & Slavkovic, A. (2018). General and specific utility measures for synthetic data. Journal of the Royal Statistical Society: Series A (Statistics in Society), 181(3), 663-688.
[link](https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12358)

Bowen, C. M., Bryant, V., Burman, L., Czajka, J., Khitatrakun, S., MacDonald, G., ... & Zwiefel, N. (2022). Synthetic Individual Income Tax Data: Methodology, Utility, and Privacy Implications. In International Conference on Privacy in Statistical Databases (pp. 191-204). Springer, Cham.
[link](https://link.springer.com/chapter/10.1007/978-3-031-13945-1_14)

Raghunathan, T. E. (2021). Synthetic data. Annual Review of Statistics and Its Application, 8, 129-140.
[link](https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-040720-031848)

<br> <br>

# References